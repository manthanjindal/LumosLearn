{
  "title": "Self-Attention",
  "content": "<b>Self-Attention</b> (also known as intra-attention) is a specific type of attention mechanism that relates different positions of a <span style=\"color:#A3F7BF\">single sequence</span> to compute a representation of the same sequence. Unlike traditional attention which focuses on an input sequence to generate an output sequence, self-attention helps a model understand the relationships between different words within the same sentence.<br><br>For example, in the sentence \"The animal didn't cross the street because it was too tired\", self-attention helps the model understand that \"it\" refers to \"the animal\". It does this by calculating attention weights between \"it\" and every other word in the sentence, giving a higher weight to \"the animal\".<br><br>The core idea is to compute a weighted sum of all elements in the input sequence for each element, where the weights are learned based on the <span style=\"color:#A3F7BF\">relevance between elements</span>. This allows the model to capture long-range dependencies within a sequence without relying on recurrence (like RNNs) or convolutions (like CNNs). Self-attention is a fundamental building block of the <span style=\"color:#A3F7BF\">Transformer architecture</span>.",
  "quiz": [
    {
      "question": "What does Self-Attention primarily focus on?",
      "options": [
        "Relating different positions within a single sequence",
        "Relating an input sequence to an output sequence",
        "Generating new sequences from noise",
        "Classifying images into categories"
      ],
      "answer": 0
    },
    {
      "question": "Which neural network architecture is built entirely on Self-Attention?",
      "options": [
        "Recurrent Neural Networks (RNNs)",
        "Convolutional Neural Networks (CNNs)",
        "Long Short-Term Memory (LSTM) networks",
        "Transformers"
      ],
      "answer": 3
    },
    {
      "question": "What is a key benefit of Self-Attention regarding dependencies?",
      "options": [
        "It only captures short-term dependencies",
        "It captures long-range dependencies without recurrence or convolution",
        "It requires more manual feature engineering",
        "It makes the model less interpretable"
      ],
      "answer": 1
    }
  ],
  "title_hi": "आत्मनिर्णय",
  "content_hi": "<b> सेल्फ-एटीबेशन </b> (जिसे इंट्रा-अटेंशन के रूप में भी जाना जाता है) एक विशिष्ट प्रकार का ध्यान तंत्र है जो एक <स्पैन स्टाइल = \"रंग:#A3F7BF\"> सिंगल अनुक्रम </स्पैन> के विभिन्न पदों से संबंधित है, एक ही अनुक्रम के प्रतिनिधित्व की गणना करने के लिए। पारंपरिक ध्यान के विपरीत, जो एक आउटपुट अनुक्रम उत्पन्न करने के लिए एक इनपुट अनुक्रम पर ध्यान केंद्रित करता है, आत्म-क्षीणन एक मॉ��ल को एक ही वाक्य के भीतर विभिन्न शब्दों के बीच संबंधों को समझने में मदद करता है। उदाहरण के लिए, उदाहरण के लिए, वाक्य में \"जानवर सड़क को पार नहीं करता था क्योंकि यह बहुत थका हुआ था\", स्व-अटेंडेंट मॉडल को समझने में मदद करता है कि \"यह\" जानवर \"को संदर्भित करता है। यह वाक्य में \"इट\" और हर दूसरे शब्द के बीच ध्यान देने वाले वजन की गणना करके करता है, \"जानवर\" को एक उच्च वजन देता है। <br> <br> <br> कोर विचार प्रत्येक तत्व के लिए इनपुट अनुक्रम में सभी तत्वों के भारित योग की गणना करना है, जहां भार को <स्पैन स्टाइल = \"रंग:#A3F7BF\"> प्रासंगिकता के आधार पर सीखा जाता है। यह मॉडल को पुनरावृत्ति (जैसे RNNs) या Confolutions (जैसे CNNs) पर भरोसा किए बिना एक अनुक्रम के भीतर लंबी दूरी की निर्भरता को कैप्चर करने की अनुमति देता है। सेल्फ-एब्जीशन <स्पैन स्टाइल = \"कलर:#ए 3 एफ 7 बीएफ\"> ट्रांसफार्मर आर्किटेक्चर </स्पैन> का एक मौलिक बिल्डिंग ब्लॉक है।",
  "quiz_hi": [
    {
      "question": "स्व-संयोग मुख्य रूप से क्या ध्यान केंद्रित करता है?",
      "options": [
        "एक ही अनुक्रम के भीतर विभिन्न पदों से संबंधित",
        "एक आउटपुट अनुक्रम के लिए एक इनपुट अनुक्रम से संबंधित",
        "शोर से नए अनुक्रम उत्पन्न करना",
        "श्रेणियों में छवियों को वर्गीकृत करना"
      ],
      "answer": 0
    },
    {
      "question": "कौन सी न्यूरल नेटवर्क आर्किटेक्चर पूरी तरह से आत्म-ध्यान पर है?",
      "options": [
        "आवर्तक तंत्रिका नेटवर्क (आरएनएन)",
        "संकल्प तंत्रिका नेटवर्क (सीएनएन)",
        "लंबी अल्पकालिक मेमोरी (LSTM) नेटवर्क",
        "ट्रान्सफ़ॉर्मर"
      ],
      "answer": 3
    },
    {
      "question": "निर्भरता के संबंध में आत्म-संलग्नन का एक प्रमुख लाभ क्या है?",
      "options": [
        "यह केवल अल्पकालिक निर्भरता को कैप्चर करता है",
        "यह पुनरावृत्ति या दृढ़ संकल्प के बिना लंबी दूरी की निर्भरता को कैप्चर करता है",
        "इसके लिए अधिक मैनुअल फीचर इंजीनियरिंग की आवश्यकता होती है",
        "यह मॉडल को कम व्याख्या योग्य बनाता है"
      ],
      "answer": 1
    }
  ]
}