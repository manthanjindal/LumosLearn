{
  "title": "GRU: Theory",
  "content": "The <span style=\"color:#A3F7BF\">Gated Recurrent Unit (GRU)</span> is another type of recurrent neural network that, like LSTM, was designed to address the <span style=\"color:#A3F7BF\">vanishing gradient problem</span> and improve the memory of RNNs for sequential data.<br><br>Introduced in 2014, GRUs are considered a <span style=\"color:#A3F7BF\">simpler alternative to LSTMs</span>. While LSTMs have three gates (input, forget, output) and a separate cell state, GRUs combine the cell state and hidden state, and they only have two gates:<br><br><i>1. <b>Update Gate:</b></i> Controls how much of the past information (from the previous hidden state) should be carried over to the current hidden state. It acts like both the input and forget gates of an LSTM.<br><i>2. <b>Reset Gate:</b></i> Determines how much of the previous hidden state to <span style=\"color:#A3F7BF\">forget</span>. It decides how to combine the new input with the previous memory.<br><br>The simplicity of GRUs means they have <span style=\"color:#A3F7BF\">fewer parameters</span> than LSTMs, which can lead to faster training times and sometimes better performance on smaller datasets, while still being effective at capturing long-term dependencies.",
  "quiz": [
    {
      "question": "What problem do GRUs, like LSTMs, aim to solve in RNNs?",
      "options": [
        "Overfitting",
        "Exploding gradients",
        "Vanishing gradient problem",
        "Computational inefficiency"
      ],
      "answer": 2
    },
    {
      "question": "How many gates does a Gated Recurrent Unit (GRU) typically have?",
      "options": [
        "One",
        "Two",
        "Three",
        "Four"
      ],
      "answer": 1
    },
    {
      "question": "What is a potential advantage of GRUs over LSTMs?",
      "options": [
        "They are more complex",
        "They have more parameters",
        "They can lead to faster training times due to fewer parameters",
        "They cannot handle long-term dependencies"
      ],
      "answer": 2
    }
  ],
  "title_hi": "जीआरयू: सिद्धांत",
  "content_hi": "<स्पैन स्टाइल = \"रंग:#a3f7bf\"> गेटेड आवर्तक इकाई (GRU) </span> एक अन्य प्रकार का आवर्तक तंत्रिका नेटवर्क है, जैसे कि LSTM, को <स्पैन स्टाइल = \"रंग:#A3F7BF\"> वैनिशिंग ग्रेडिएंट प्रॉब्लम </span> को संबोधित करने के लिए डिज़ाइन किया गया था। style = \"रंग:#a3f7bf\"> lstms </span> का सरल विकल्प। जबकि LSTM में तीन गेट्स (इनपुट, भूल, आउटपुट) और एक अलग सेल स्थिति है, GRUS सेल स्टेट और हिडन स्टेट को मिलाकर, और उनके पास केवल दो गेट हैं: <br> <br> <i> 1। <b> अद्यतन गेट: </b> </i> नियंत्रित करता है कि पिछली जानकारी (पिछली छिपी हुई अवस्था से) को वर्तमान छिपी हुई स्थिति में ले जाया जाना चाहिए। यह इनपुट दोनों की तरह काम करता है और एक LSTM के द्वा�� को भूल जाता है। <br> <i> 2। <b> रीसेट गेट: </b> </i> यह निर्धारित करता है कि पिछली छिपी हुई अवस्था में <स्पैन स्टाइल = \"रंग:#a3f7bf\"> को भूल जाओ </span>। यह तय करता है कि पिछली मेमोरी के साथ नए इनपुट को कैसे संयोजित किया जाए। <br> <br> <br> GRUS की सादगी का मतलब है कि उनके पास <स्पैन स्टाइल = \"रंग:#A3F7BF\"> कम पैरामीटर </span> LSTM की तुलना में तेजी से प्रशिक्षण समय और कभी-कभी छोटे डेटासेट पर बेहतर प्रदर्शन हो सकता है, जबकि अभी भी लंबे समय तक निर्भरता ���ो कैप्चर करने में प्रभावी है।",
  "quiz_hi": [
    {
      "question": "LSTMs की तरह ग्रूस क्या समस्या है, इसका उद्देश्य RNNs में हल करना है?",
      "options": [
        "अतिप्रवाह",
        "विस्फोट ग्रेडिएंट्स",
        "गायब होने की समस्या",
        "कम्प्यूटेशनल अक्षमता"
      ],
      "answer": 2
    },
    {
      "question": "आमतौर पर एक गेटेड आवर्तक इकाई (GRU) में कितने गेट होते हैं?",
      "options": [
        "एक",
        "दो",
        "तीन",
        "चार"
      ],
      "answer": 1
    },
    {
      "question": "LSTMS पर GRUS का संभावित लाभ क्या है?",
      "options": [
        "वे अधिक जटिल हैं",
        "उनके पास अधिक पैरामीटर हैं",
        "वे कम मापदंडों के कारण तेजी से प्रशिक्षण समय का कारण बन सकते हैं",
        "वे दीर्घकालिक निर्भरता को संभाल नहीं सकते"
      ],
      "answer": 2
    }
  ]
}