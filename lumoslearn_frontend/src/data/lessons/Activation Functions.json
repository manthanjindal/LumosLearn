{
  "title": "Activation Functions",
  "content": "An <span style=\"color:#A3F7BF\">activation function</span> is a crucial component of an artificial neural network, placed at the output of each neuron. Its primary purpose is to introduce <span style=\"color:#A3F7BF\">non-linearity</span> into the network.<br><br>Without activation functions, a neural network, no matter how many layers it has, would simply be performing linear transformations, making it equivalent to a single-layer perceptron. Non-linearity allows the network to learn complex patterns and map non-linear relationships between inputs and outputs.<br><br>Common activation functions include:<br><i>1. <b>Sigmoid:</b></i> Squashes values between 0 and 1, often used in output layers for binary classification.<br><i>2. <b>ReLU (Rectified Linear Unit):</b></i> Outputs the input directly if it's positive, otherwise outputs zero. Very popular in hidden layers due to its computational efficiency.<br><i>3. <b>Tanh (Hyperbolic Tangent):</b></i> Squashes values between -1 and 1.<br><i>4. <b>Softmax:</b></i> Used in the output layer for multi-class classification, converting outputs into probabilities that sum to 1.",
  "quiz": [
    {
      "question": "What is the primary purpose of an activation function in a neural network?",
      "options": [
        "To normalize input data",
        "To introduce non-linearity",
        "To reduce the number of layers",
        "To calculate the loss"
      ],
      "answer": 1
    },
    {
      "question": "Which activation function outputs the input directly if positive, otherwise zero?",
      "options": [
        "Sigmoid",
        "Tanh",
        "Softmax",
        "ReLU"
      ],
      "answer": 3
    },
    {
      "question": "If a neural network did not use activation functions, what would be its limitation?",
      "options": [
        "It could only solve classification problems",
        "It would be equivalent to a single-layer perceptron",
        "It would require more training data",
        "It would be unable to perform forward propagation"
      ],
      "answer": 1
    }
  ],
  "title_hi": "सक्रियण कार्य",
  "content_hi": "एक <स्पैन स्टाइल = \"रंग:#a3f7bf\"> सक्रियण फ़ंक्शन </span> एक कृत्रिम तंत्रिका नेटवर्क का एक महत्वपूर्ण घटक है, जिसे प्रत्येक न्यूरॉन के आउटपुट में रखा गया है। इसका प्राथमिक उद्देश्य नेटवर्क में <स्पैन स्टाइल = \"रंग:#a3f7bf\"> गैर-रैखिकता </span> का परिचय देना है। <br> <br> <br> <br> बिना सक्रियण कार्यों के, एक तंत्रिका नेटवर्क, कोई फर्क नहीं पड��ता कि यह कितनी परत है, बस रैखिक परिवर्तनों का प्रदर्शन कर रहा होगा, यह एक एकल-लेयर पेरसेपट्रॉन के बराबर है। गैर-रैखिकता नेटवर्क को जटिल पैटर्न सीखने और इनपुट और आउटपुट के बीच गैर-रैखिक संबंधों को सीखने की अनुमति देती है। <br> <br> सामान्य सक्रियण कार्यों में शामिल हैं: <br> <i> 1। <b> सिग्मॉइड: </b> </i> 0 और 1 के बीच स्क्वैश मान, अक्सर बाइनरी वर्गीकरण के लिए आउटपुट लेयर्स में उपयोग किया जाता है। <br> <i> 2। <b> relu (refied रैखिक इकाई): </b> </i> यदि यह सकारात्मक है, तो सीधे इनपुट को आउटपुट करता है, अन्यथा शून्य आउटपुट करता है। इसकी कम्प्यूटेशनल दक्षता के कारण छिपी हुई परतों में बहुत लोकप्रिय है। <br> <i> 3। <b> तन्ह (हाइपरबोलिक स्पर्शरेखा): </b> </i> -1 और 1. <br> <i> 4 के बीच मान स्क्वैश। <b> सॉफ्टमैक्स: </b> </i> मल्टी-क्लास वर्गीकरण के लिए आउटपुट लेयर में उपयोग किया जाता है, आउटपुट को संभावनाओं में परिवर्तित करता है जो 1 में योग करता है।",
  "quiz_hi": [
    {
      "question": "एक तंत्रिका नेटवर्क में एक सक्रियण फ़ंक्शन का प्राथमिक उद्देश्य क्या है?",
      "options": [
        "इनपुट डेटा को सामान्य करने के लिए",
        "गैर-रैखिकता का परिचय देना",
        "परतों की संख्या को कम करने के लिए",
        "नुकसान की गणना करने के लिए"
      ],
      "answer": 1
    },
    {
      "question": "कौन सा सक्रियण फ़ंक्शन इनपुट को सीधे आउटपुट करता है यदि सकारात्मक, अन्यथा शून्य?",
      "options": [
        "अवग्रह",
        "तन्ह",
        "सॉफ्टमैक्स",
        "फिर शुरू करना"
      ],
      "answer": 3
    },
    {
      "question": "यदि एक तंत्रिका नेटवर्क ने सक्रियण कार्यों का उपयोग नहीं किया, तो इसकी सीमा क्या होगी?",
      "options": [
        "यह केवल वर्गीकरण समस्याओं को हल कर सकता है",
        "यह एक एकल-परत पर्सपट्रॉन के बराबर होगा",
        "इसके लिए अधिक प्रशिक्षण डेटा की आवश्यकता होगी",
        "यह आगे का प्रसार करने में असमर्थ होगा"
      ],
      "answer": 1
    }
  ]
}